\documentclass[3p,review]{elsarticle}

\usepackage{lineno,hyperref}
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet} % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
%\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS


\usepackage{graphics, epstopdf, epsfig}
\usepackage{amstext}
\usepackage{subcaption}
%\usepackage{color}
%\usepackage{indentfirst}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage{rotating}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{fixltx2e}
\usepackage{xcolor}
%\usepackage{lipsum,graphicx,subcaption}
\usepackage{xspace}

\newcommand{\ie}{\emph{i.e.,}\xspace}
\newcommand{\eg}{\emph{e.g.,}\xspace}
\newcommand{\etal}{\emph{et al.}\xspace}
\newcommand{\paratitle}[1]{\vspace{1em}\noindent \textbf{#1}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}  % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm

%
%\makeatletter  
%\newif\if@restonecol  
%\makeatother  
%\let\algorithm\relax  
%\let\endalgorithm\relax  
%\usepackage[linesnumbered,ruled,vlined]{algorithm2e}%[ruled,vlined]{  



\captionsetup[subfigure]{labelformat=simple,labelsep=colon}
\renewcommand{\thesubfigure}{}

\modulolinenumbers[5]

\journal{Journal of \LaTeX\ Templates}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
%\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{adversarial text mining for speculative sentiment classification }
%\tnotetext[mytitlenote]{Fully documented templates are available in the elsarticle package on \href{http://www.ctan.org/tex-archive/macros/latex/contrib/elsarticle}{CTAN}.}

%% Group authors per affiliation:
%\author{Jiahui Wen\fnref{myfootnote}}
%\address{Radarweg 29, Amsterdam}
%\fntext[myfootnote]{Since 1880.}

% \author[address1]{Hongkui Tu}
% \ead{tuhkjet@foxmail.com}

% \author[address1]{Jiahui Wen\corref{mycorrespondingauthor}}
% \ead{wen\_jiahui@outlook.com}
% \cortext[mycorrespondingauthor]{Corresponding author}

% %% or include affiliations in footnotes:
% \author[address1]{Guangda Zhang}
% \ead{zhanggd\_nudt@hotmail.com}



% \address[address1]{National Innovative Institute of Defense Technology, Beijing, China}
% \address[address3]{National University of Defense Technology, Changsha, China}

\begin{abstract}

\end{abstract}

\begin{keyword}
% collaborative sentiment classification; speculative similar document
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction}

% 之前的工作建立在文本相似理论的基础上，文本相似理论任务具有相似特征的文本在概率上更有可能属于同一个情感类别。这种理论忽略了一个情感类别的真实文本分布，对于处于边界的文本更具有错误叠加的风险。



% 给定一个文档，生成器的目的是找到一些文档，使得这些文档和给定的文档具有相同标注或者类别，所以当对一个特定文档的类别不是很确定的时候，可以通过生成器找到与其具有相同类别的文档，通过对这些文档的类别，我们可以更为容易地确定该给定文档地类别。
GAN utilizes the minimax game theory to generate plausible fake data samples, and has been commonly applied in data augmentation areas such as images, video and texts. Unlike the vanilla GAN that has no control on modes of the data being generated, the proposed model is flexible enough to condition on arbitrary information that is necessary to draw the speculative similar documents (SSD should be explained before). Specifically, the proposed model consist of two components: generator, discriminator and collaborator. For the generator, given an anchor, the goal is to sample candidate documents from the repository. the sampling of the candidate documents are conditioned on necessary information such as class (similar or dissimilar), and the user, item and texts of the anchor document. Therefore, a well-trained generator is expected to output the most plausible documents to our advantage. For example, when the class is set to 1 (similar), the generator can output documents that have the same ground-truth sentiment as the anchor document, and we can incorporate those speculative similar documents for improving classification performance. The discriminator, on the contrary, plays the role of a classifier that focuses on discriminating whether an document is fake from the generator or is real from the original dataset. The generator and the discriminator are trained in an adversarial manner until an equilibrium state is reached, where the discriminator can no longer distinguish whether a document is fake or real. Finally, the collaborator incorporates the speculative similar documents provided by the generator into a collaborative filtering framework, and learns better representations for the users, items and documents, which in turn benefits the learning of generator and discriminator.


for a given input $<u_i, v_j, d_{ij}>$, where $d_{ij}$ denotes the document that user $u_i$ writes about $v_j$, the primary goal of the proposed model is to determine the overall sentiment of $d_{ij}$. For speculative sentiment classification, one subgoal is



for a candidate document, the discriminator simultaneously determine

for discriminator:

\begin{equation}\label{eq:gen}
\mathbb{E}_{d_k\sim P(d|d_{ij},s)}log[D_{\phi}(d_k,d_{ij},s)]+\mathbb{E}_{d_k'\sim P_{G_{\theta}(d|d_{ij},s)}}log[D_{\phi}(d_k',d_{ij},s)]
\end{equation}

$\mathbb{E}$

\begin{equation}\label{eq:d1}
  \mathcal{L}_1^D=\mathbb{E}_{d_k\sim P(d|d_{ij},s)}-logf_1^D(d_k,d_{ij})
\end{equation}

\begin{equation}\label{eq:d2}
  \mathcal{L}_2^D=\mathbb{E}_{d_k'\sim P_{G_{\theta}(d|d_{ij},s)}}-log[1-f_1^D(d_k',d_{ij})]
\end{equation}


\begin{equation}\label{eq:d1}
  \mathcal{L}_3^D=\mathbb{E}_{d_k\sim P(d|d_{ij},s)}[s*log f_2^D(d_k,d_{ij})+(1-s)*log(1-f_2^D(d_k,d_{ij}))]
\end{equation}


\begin{equation}\label{eq:d1}
  \mathcal{L}_4^D=\mathbb{E}_{d_k'\sim P_{G_{\theta}(d|d_{ij},s)}}[s*log f_2^D(d_k',d_{ij})+(1-s)*log(1-f_2^D(d_k',d_{ij}))]
\end{equation}


for generator:

\begin{equation}\label{eq:g1}
  \mathcal{L}_1^G=\mathbb{E}_{d_k'\sim P_{G_{\theta}(d|d_{ij},s)}}-logf_1^D(d_k',d_{ij})
\end{equation}

\begin{equation}\label{eq:g1}
  \mathcal{L}_2^G=\mathbb{E}_{d_k'\sim P_{G_{\theta}(d|d_{ij},s)}}[s*log f_2^D(d_k',d_{ij})+(1-s)*log(1-f_2^D(d_k',d_{ij}))] )
\end{equation}

for distribution $P_{G_{\theta}(d|d_{ij},s)}$:

\begin{equation}\label{eq:pg}
  P_{G_{\theta}(d|d_{ij},s)}=\frac{exp f^G(d_{ij},d,s)}{\sum_{k=1}^Kexp f^G(d_{ij}, d_k, s)}
\end{equation}


the generated documents have to satisfy two requirements, (1)semantically similar to the anchor document, (2) share the same sentiment with the anchor document.

\section{Conclusion}
\section{policy gradient}



\begin{equation}\label{eq:p}
  R(\theta)=\sum_{s\in\{0,1\}}P_{\theta}(s|d_{ij},d_k)V(s,d_{ij},d{k})
\end{equation}



\section{dcu}
%\begin{algorithm}
%\begin{algorithmic}[1]
%\While{not end}
%{
%	pop req_pkt from fifo;
%}
%
%
%\end{algorithmic}
%\end{algorithm}

%\begin{algorithm}  
%\While{not end}
%{
%	\If{not l2c\_noc4\_fifo.empty()}
%	{
%		$req\_pkt \leftarrow l2c\_noc4\_fifo.pop()$;
%	}
%	\ElseIf{not $l2c\_noc2\_fifo.empty$ \&\& not $l2c\_noc3\_fifo.nearfull()$}
%	{
%		$req\_pkt \leftarrow l2c\_noc2\_fifo.pop()$;
%	}
%%	\Elseif{not $req_buf.empty()$ and ~ $l2c\_noc1\_fifo.nearfull()$}
%%	{
%%		$req\_pkt \leftarrow req\_buf.pop()$;
%%	}
%%	\Else
%%	{
%%		$req\_pkt \leftarrow None$;
%%	}
%}
%%  $con(r_i)= \Phi$\;  
%%  \For{$j=1;j \le n;j \ne i$}  
%%  {  
%%    float $maxSim=0$\;  
%%    $r^{maxSim}=null$\;  
%%    \While{not end of $T_j$}  
%%    {  
%%      compute Jaro($r_i,r_m$)($r_m\in T_j$)\;  
%%      \If{$(Jaro(r_i,r_m) \ge \theta_r)\wedge (Jaro(r_i,r_m)\ge r^{maxSim})$}  
%%      {  
%%        replace $r^{maxSim}$ with $r_m$\;  
%%      }  
%%    }  
%%    $con(r_i)=con(r_i)\cup {r^{maxSim}}$\;  
%%  }  
%%  return $con(r_i)$\;  
%\end{algorithm}  


\begin{algorithm}
\caption{algpseudocode of l2c\_ref\_model}
\begin{algorithmic}[1]
\While{not end}
	\If{$not$ l2c\_noc4\_fifo.empty()}
		\State $req\_pkt \leftarrow$ l2c\_noc4\_fifo.pop();
	\ElsIf{$not$ l2c\_noc2\_fifo.empty() $and$  $not$ l2c\_noc3\_fifo.nearfull()}
		\State $req\_pkt \leftarrow$ l2c\_noc2\_fifo.pop();
	\EndIf
	
\EndWhile
\end{algorithmic}
\end{algorithm}

%
%\begin{algorithm}\footnotesize
%\caption{forward pass and back-propagation learning of HIGAN.}
%\label{alg:model}
%\begin{algorithmic}[1]
%  \Require
%  the training set $\mathcal{D}$; the learning rate $\eta$; the regularization parameter $\lambda$;
%  the reviews for each user $u_i$ and item $v_j$, $D_{u_i}$ and $D_{v_j}$; the number of interactive gate layer, $l_g$
%  \Ensure
%  latent factors of each user $u_i$ and item $v_j$, $\mathbf{u}_i$ and $\mathbf{v}_j$; semantic representation of each user $u_i$ and $v_j$, $\{\mathbf{d}_i^l\}_{i=1}^M$ and $\{\mathbf{d}_j^l\}_{j=1}^N$; the model parameters,$\Theta$.
%  \State Initialize all model parameters $\Theta$;
%  \While{not convergence}
%  	\State Randomly sample a tuple $(u_i,v_j,r_{ij})\in\mathcal{D}$;
%  	\State Obtain the user and item embeddings;
%  	\State Obtain word embeddings for each review in $D_{u_i}$ and $D_{v_j}$;
%  	\State Calculate review representations $\{\mathbf{d}_{i,k}\}_{k=1}^m$ and $\{\mathbf{d}_{j,t}\}_{t=1}^n$  as specified in Section.\ref{sec:word-level};
%  	\State Calculate initial semantic representations of the user and item, $\mathbf{d}_i^0$ and $\mathbf{d}_j^0$ by Eqn.\ref{eq:euc}, Eqn.\ref{eq:euc_min} and Eqn.\ref{eq:init_semantic};
%  	\For{$l=1$;$l<=l_g$;$l++$}
%  	   \State Calculate attentive semantic representation, $\mathbf{\widetilde{d}}_{i}^l, \mathbf{\widetilde{d}}_{j}^l $, through an mutual attention layer by Eqn.\ref{eqn:att1}, Eqn.\ref{eqn:att2};
%  	   \State Calculate new semantic representation, $\mathbf{d}_i^{l}, \mathbf{d}_j^l$, through a gate layer by Eqn.\ref{eqn:gat1}, Eqn.\ref{eqn:gat2};
%  	\EndFor
%  	\State Calculate the predicted score $\hat{r}_{ij}$ as descried in Section.\ref{sec:prediction};
%  	\State Calculate $\frac{\partial\mathcal{L}}{\partial\Theta}$ and update $\Theta$ by Eqn.\ref{eq:learn};
%  \EndWhile
%\end{algorithmic}
%\end{algorithm}




\bibliographystyle{elsarticle-num}
% \bibliography{bib}


\end{document}\grid
